{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8U7or8th5ml"
      },
      "source": [
        "##Nama kelompok 2\n",
        "\n",
        "1.Enjel Devita Hutauruk\\\n",
        "2.Armando Mulana Putra\\\n",
        "3.Mila Delvana\\\n",
        "4.Henny Kristianing putri\\\n",
        "5.Yultriyen\\\n",
        "6.Dwi Khairani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou5MMmzfRNv9"
      },
      "source": [
        "# **Chatbot Untuk Sewa Mobil**\n",
        "\n",
        "<b>Chatbot</b> merupakan bagian dari Natural Language Processing yang digunakan untuk percakapan dengan pengguna melalui teks dan ucapan. Pada mini proyek ini kami akan membuat sebuah chat otomatis atau chatbot tentang program pemesanan sewa mobil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pieKAcQNDniR"
      },
      "source": [
        "# **Load Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZMu0fUzB237"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7fv7xUWCs4Q"
      },
      "outputs": [],
      "source": [
        "!pip -q install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMMP5-liFFur"
      },
      "outputs": [],
      "source": [
        "!pip -q install gtts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGl6pk_DFMtO"
      },
      "source": [
        "# **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfRv5iZgFOlC"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gtts import gTTS\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import speech_recognition as sr \n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Flatten, Dense, GlobalMaxPool1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWM45B8eUopG"
      },
      "outputs": [],
      "source": [
        "# Package sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "# Package lemmatization\n",
        "nltk.download('wordnet')\n",
        "# Package multilingual wordnet data\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Vy3rdnYPH3"
      },
      "source": [
        "# **Load Dataset Json**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKFVHri7IvU0"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "with open('/content/drive/MyDrive/Proyek/Natural Processing/sewa.json') as content:\n",
        "  data1 = json.load(content)\n",
        "\n",
        "# Mendapatkan semua data ke dalam list\n",
        "tags = []\n",
        "inputs=[]\n",
        "responses={}\n",
        "words=[]\n",
        "classes=[]\n",
        "documents=[]\n",
        "ignore_words=['?', '!']\n",
        "\n",
        "\n",
        "for intent in data1['pengetahuan']:\n",
        "  responses[intent['tag']]=intent['responses']\n",
        "  for lines in intent['patterns']:\n",
        "    inputs.append(lines)\n",
        "    tags.append(intent['tag'])\n",
        "    for pattern in intent['patterns']:\n",
        "      w = nltk.word_tokenize(pattern)\n",
        "      words.extend(w)\n",
        "      documents.append((w, intent['tag']))\n",
        "      # add to our classes list\n",
        "      if intent['tag'] not in classes:\n",
        "        classes.append(intent['tag'])\n",
        "\n",
        "# Konversi data json ke dalam dataframe\n",
        "data = pd.DataFrame({\"patterns\":inputs, \"tags\":tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QtVG2xZbt_"
      },
      "source": [
        "# **Preprocessing The Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNT9hrf-ZoLs"
      },
      "source": [
        "## **Remove Punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ead13_GVZROc"
      },
      "outputs": [],
      "source": [
        "# Removing Punctuations (Menghilangkan Punktuasi)\n",
        "data['patterns']=data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
        "data['patterns']=data['patterns'].apply(lambda wrd: ''.join(wrd)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGd-sQYVZwLy"
      },
      "source": [
        "## **Lemmatization (Lematisasi)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NSaa_lVbgxE"
      },
      "outputs": [],
      "source": [
        "# Lematisasi atau Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "print (len(words), \"unique lemmatized words\", words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q8sHqz0bwjk"
      },
      "source": [
        "### **Menyortir Data Kelas Tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSzTwXSTb4i5"
      },
      "outputs": [],
      "source": [
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(classes), \"classes\", classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t41xF_Ndb6ez"
      },
      "source": [
        "### **Mencari Jumlah Keseluruhan Data Teks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J3SY-7Eb66S"
      },
      "outputs": [],
      "source": [
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhQIfBmZZ3j8"
      },
      "source": [
        "## **Tokenization (Tokenisasi)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI7ecHjCcBWv"
      },
      "outputs": [],
      "source": [
        "# Tokenize the data (Tokenisasi Data)\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(data['patterns'])\n",
        "train = tokenizer.texts_to_sequences(data['patterns'])\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5EKXE0IZ-RK"
      },
      "source": [
        "## **Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js1KsqqlcTvJ"
      },
      "outputs": [],
      "source": [
        "# Apply padding \n",
        "x_train = pad_sequences(train)\n",
        "print(x_train) # Padding Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HSO-PyvaHSJ"
      },
      "source": [
        "## **Encoding Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmvvXOqNdQJF"
      },
      "outputs": [],
      "source": [
        "# Encoding the outputs \n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(data['tags'])\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckVXCm5rdyrw"
      },
      "source": [
        "# **Input Length, Output Length and Vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEp803J_d1Zt"
      },
      "outputs": [],
      "source": [
        "# input length\n",
        "input_shape = x_train.shape[1]\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdKw0tCoeACb"
      },
      "outputs": [],
      "source": [
        "# define vocabulary\n",
        "vocabulary = len(tokenizer.word_index)\n",
        "print(\"number of unique words : \", vocabulary)\n",
        "\n",
        "# output length\n",
        "output_length = le.classes_.shape[0]\n",
        "print(\"output length: \", output_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THZGu-iRaRU5"
      },
      "source": [
        "## **Save Model Words & Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QMiVYcJecj3"
      },
      "outputs": [],
      "source": [
        "pickle.dump(words, open('/content/drive/MyDrive/Proyek/Natural Processing/model/words.pkl','wb'))\n",
        "pickle.dump(classes, open('/content/drive/MyDrive/Proyek/Natural Processing/model/classes.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_SkVQxegIw"
      },
      "source": [
        "## **Save Label Encoder & Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ2aawgJelMl"
      },
      "outputs": [],
      "source": [
        "pickle.dump(le, open('le.pkl','wb'))\n",
        "pickle.dump(tokenizer, open('tokenizers.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APUv-y_ratvr"
      },
      "source": [
        "# **Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWBfM-9Ne5FC"
      },
      "outputs": [],
      "source": [
        "# Creating the model (Membuat Modeling)\n",
        "i = Input(shape=(input_shape,))\n",
        "x = Embedding(vocabulary+1,10)(i) # Layer Embedding\n",
        "x = LSTM(10, return_sequences=True)(x) # Layer Long Short Term Memory\n",
        "x = Flatten()(x) # Layer Flatten\n",
        "x = Dense(output_length, activation=\"softmax\")(x) # Layer Dense\n",
        "model  = Model(i,x)\n",
        "\n",
        "# Compiling the model (Kompilasi Model)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88W7kfYutM7S"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpUcIIpEfXvq"
      },
      "outputs": [],
      "source": [
        "# Training the model (Latih model data sampai 400 kali)\n",
        "train = model.fit(x_train, y_train, epochs=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XitcsLsayUA"
      },
      "source": [
        "# **Model Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UFmsQUpgcBR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train.history['accuracy'],label='Training Set Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train.history['loss'],label='Training Set Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2XNvrF7a5Bq"
      },
      "source": [
        "# **Testing Chatbot Dan Tambahkan Suara Pada Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GWajt4uf0LH"
      },
      "outputs": [],
      "source": [
        "# Membuat Input Chat\n",
        "while True:\n",
        "  texts_p = []\n",
        "  prediction_input = input('Kamu : ')\n",
        "  \n",
        "  # Menghapus punktuasi dan konversi ke huruf kecil\n",
        "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
        "  prediction_input = ''.join(prediction_input)\n",
        "  texts_p.append(prediction_input)\n",
        "  # Tokenisasi dan Padding\n",
        "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
        "  prediction_input = np.array(prediction_input).reshape(-1)\n",
        "  prediction_input = pad_sequences([prediction_input], input_shape)\n",
        "  # Mendapatkan hasil keluaran pada model \n",
        "  output = model.predict(prediction_input)\n",
        "  output = output.argmax()\n",
        "\n",
        "  # Menemukan respon sesuai data tag dan memainkan voice bot\n",
        "  response_tag = le.inverse_transform([output])[0]\n",
        "  print(\"ðŸ¤– Sewabot : \", random.choice(responses[response_tag]))\n",
        "  tts = gTTS(random.choice(responses[response_tag]), lang='id')\n",
        "  # Simpan model voice bot ke dalam Google Drive\n",
        "  tts.save('/content/drive/MyDrive/Proyek/Natural Processing/sewabot.wav')\n",
        "  time.sleep(0.08)\n",
        "  # Load model voice bot from Google Drive\n",
        "  ipd.display(ipd.Audio('/content/drive/MyDrive/Proyek/Natural Processing/sewabot.wav', autoplay=False))\n",
        "  print(\"=\"*60 + \"\\n\")\n",
        "  # Tambahkan respon 'berpisah' agar bot bisa berhenti\n",
        "  if response_tag == \"berpisah\":\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEJzItnXi70Z"
      },
      "source": [
        "# **Save The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y74Y7oTKijAU"
      },
      "outputs": [],
      "source": [
        "# Simpan model dalam bentuk format file .h5 atau .pkl (pickle)\n",
        "model.save('/content/drive/MyDrive/Proyek/Natural Processing/model/chat_model.h5')\n",
        "\n",
        "print('Model Created Successfully!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
